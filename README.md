# PromptFoo Quickstart: Funny Tweet Evaluator

This repo shows how to use PromptFoo to evaluate and compare responses from different LLMs using prompt variations and automated scoring.

This example compares OpenAI's gpt-4o and gpt-4o-mini on their ability to generate funny tweets about different topics.

PromptFoo is a command-line tool and framework for:

✅ Testing and improving prompt quality

✅ Comparing model outputs across providers (OpenAI, Claude, HuggingFace, etc.)

✅ Defining rules and rubrics to automatically grade responses

✅ Running evaluations via terminal or browser

✅ Tracking prompt performance over time

Use it to systematically benchmark prompts for use cases like summarization, classification, creative writing, RAG, or agent chains.
